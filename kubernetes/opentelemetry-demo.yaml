# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0
# This file is generated by 'make generate-kubernetes-manifests'
---
apiVersion: v1
kind: Namespace
metadata:
  name: otel-demo
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "clickhouse-pdb"
  labels:
    app.kubernetes.io/name: clickhouse
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.19.0"
    app.kubernetes.io/component: clickhouse
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clickhouse
      app.kubernetes.io/instance: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
  name: grafana
  namespace: otel-demo
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jaeger
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: all-in-one
automountServiceAccountToken: true
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.120.0"
    app.kubernetes.io/component: standalone-collector
---
# Source: opentelemetry-demo/charts/prometheus/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v3.1.0
    app.kubernetes.io/part-of: prometheus
  name: prometheus
  namespace: otel-demo
  annotations:
    {}
---
# Source: opentelemetry-demo/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-demo
  labels:
    
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: grafana
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
type: Opaque
data:
  
  admin-user: "YWRtaW4="
  admin-password: "YWRtaW4="
  ldap-toml: ""
---
# Source: opentelemetry-demo/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
data:
  
  plugins: grafana-clickhouse-datasource
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [auth]
    disable_login_form = true
    [auth.anonymous]
    enabled = true
    org_name = Main Org.
    org_role = Admin
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
    root_url = %(protocol)s://%(domain)s:%(http_port)s/grafana
    serve_from_sub_path = true
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - editable: true
      isDefault: true
      jsonData:
        exemplarTraceIdDestinations:
        - datasourceUid: webstore-traces
          name: trace_id
        - name: trace_id
          url: http://localhost:8080/jaeger/ui/trace/$${__value.raw}
          urlDisplayLabel: View in Jaeger UI
      name: Prometheus
      type: prometheus
      uid: webstore-metrics
      url: http://prometheus:9090
    - editable: true
      isDefault: false
      name: Jaeger
      type: jaeger
      uid: webstore-traces
      url: http://jaeger-query:16686/jaeger/ui
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: false
      editable: true
      folder: ""
      name: default
      options:
        path: /var/lib/grafana/dashboards/default
      orgId: 1
      type: file
---
# Source: opentelemetry-demo/templates/flagd-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: flagd-config
  namespace: otel-demo
  labels:
    
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
data:
  
  demo.flagd.json: |
    {
      "$schema": "https://flagd.dev/schema/v0/flags.json",
      "flags": {
        "productCatalogFailure": {
          "description": "Fail product catalog service on a specific product",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "recommendationCacheFailure": {
          "description": "Fail recommendation service cache",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adManualGc": {
          "description": "Triggers full manual garbage collections in the ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adHighCpu": {
          "description": "Triggers high cpu load in the ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adFailure": {
          "description": "Fail ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "kafkaQueueProblems": {
          "description": "Overloads Kafka queue while simultaneously introducing a consumer side delay leading to a lag spike",
          "state": "ENABLED",
          "variants": {
            "on": 100,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "cartFailure": {
          "description": "Fail cart service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "paymentFailure": {
          "description": "Fail payment service charge requests n%",
          "state": "ENABLED",
          "variants": {
            "100%": 1,
            "90%": 0.95,
            "75%": 0.75,
            "50%": 0.5,
            "25%": 0.25,
            "10%": 0.1,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "paymentUnreachable": {
          "description": "Payment service is unavailable",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "loadGeneratorFloodHomepage": {
          "description": "Flood the frontend with a large amount of requests.",
          "state": "ENABLED",
          "variants": {
            "on": 100,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "imageSlowLoad": {
          "description": "slow loading images in the frontend",
          "state": "ENABLED",
          "variants": {
            "10sec": 10000,
            "5sec": 5000,
            "off": 0
          },
          "defaultVariant": "off"
        }
      }
    }
---
# Source: opentelemetry-demo/templates/product-catalog-products.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: product-catalog-products
  namespace: otel-demo
  labels:
    
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
data:
  
  products.json: |
    {
      "products": [
        {
          "id": "OLJCESPC7Z",
          "name": "National Park Foundation Explorascope",
          "description": "The National Park Foundation’s (NPF) Explorascope 60AZ is a manual alt-azimuth, refractor telescope perfect for celestial viewing on the go. The NPF Explorascope 60 can view the planets, moon, star clusters and brighter deep sky objects like the Orion Nebula and Andromeda Galaxy.",
          "picture": "NationalParkFoundationExplorascope.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 101,
            "nanos": 960000000
          },
          "categories": [
            "telescopes"
          ]
        },
        {
          "id": "66VCHSJNUP",
          "name": "Starsense Explorer Refractor Telescope",
          "description": "The first telescope that uses your smartphone to analyze the night sky and calculate its position in real time. StarSense Explorer is ideal for beginners thanks to the app’s user-friendly interface and detailed tutorials. It’s like having your own personal tour guide of the night sky",
          "picture": "StarsenseExplorer.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 349,
            "nanos": 950000000
          },
          "categories": [
            "telescopes"
          ]
        },
        {
          "id": "1YMWWN1N4O",
          "name": "Eclipsmart Travel Refractor Telescope",
          "description": "Dedicated white-light solar scope for the observer on the go. The 50mm refracting solar scope uses Solar Safe, ISO compliant, full-aperture glass filter material to ensure the safest view of solar events.  The kit comes complete with everything you need, including the dedicated travel solar scope, a Solar Safe finderscope, tripod, a high quality 20mm (18x) Kellner eyepiece and a nylon backpack to carry everything in.  This Travel Solar Scope makes it easy to share the Sun as well as partial and total solar eclipses with the whole family and offers much higher magnifications than you would otherwise get using handheld solar viewers or binoculars.",
          "picture": "EclipsmartTravelRefractorTelescope.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 129,
            "nanos": 950000000
          },
          "categories": [
            "telescopes",
            "travel"
          ]
        },
        {
          "id": "L9ECAV7KIM",
          "name": "Lens Cleaning Kit",
          "description": "Wipe away dust, dirt, fingerprints and other particles on your lenses to see clearly with the Lens Cleaning Kit. This cleaning kit works on all glass and optical surfaces, including telescopes, binoculars, spotting scopes, monoculars, microscopes, and even your camera lenses, computer screens, and mobile devices.  The kit comes complete with a retractable lens brush to remove dust particles and dirt and two options to clean smudges and fingerprints off of your optics, pre-moistened lens wipes and a bottled lens cleaning fluid with soft cloth.",
          "picture": "LensCleaningKit.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 21,
            "nanos": 950000000
          },
          "categories": [
            "accessories"
          ]
        },
        {
          "id": "2ZYFJ3GM2N",
          "name": "Roof Binoculars",
          "description": "This versatile, all-around binocular is a great choice for the trail, the stadium, the arena, or just about anywhere you want a close-up view of the action without sacrificing brightness or detail. It’s an especially great companion for nature observation and bird watching, with ED glass that helps you spot the subtlest field markings and a close focus of just 6.5 feet.",
          "picture": "RoofBinoculars.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 209,
            "nanos": 950000000
          },
          "categories": [
            "binoculars"
          ]
        },
        {
          "id": "0PUK6V6EV0",
          "name": "Solar System Color Imager",
          "description": "You have your new telescope and have observed Saturn and Jupiter. Now you're ready to take the next step and start imaging them. But where do you begin? The NexImage 10 Solar System Imager is the perfect solution.",
          "picture": "SolarSystemColorImager.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 175,
            "nanos": 0
          },
          "categories": [
            "accessories",
            "telescopes"
          ]
        },
        {
          "id": "LS4PSXUNUM",
          "name": "Red Flashlight",
          "description": "This 3-in-1 device features a 3-mode red flashlight, a hand warmer, and a portable power bank for recharging your personal electronics on the go. Whether you use it to light the way at an astronomy star party, a night walk, or wildlife research, ThermoTorch 3 Astro Red’s rugged, IPX4-rated design will withstand your everyday activities.",
          "picture": "RedFlashlight.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 57,
            "nanos": 80000000
          },
          "categories": [
            "accessories",
            "flashlights"
          ]
        },
        {
          "id": "9SIQT8TOJO",
          "name": "Optical Tube Assembly",
          "description": "Capturing impressive deep-sky astroimages is easier than ever with Rowe-Ackermann Schmidt Astrograph (RASA) V2, the perfect companion to today’s top DSLR or astronomical CCD cameras. This fast, wide-field f/2.2 system allows for shorter exposure times compared to traditional f/10 astroimaging, without sacrificing resolution. Because shorter sub-exposure times are possible, your equatorial mount won’t need to accurately track over extended periods. The short focal length also lessens equatorial tracking demands. In many cases, autoguiding will not be required.",
          "picture": "OpticalTubeAssembly.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 3599,
            "nanos": 0
          },
          "categories": [
            "accessories",
            "telescopes",
            "assembly"
          ]
        },
        {
          "id": "6E92ZMYYFZ",
          "name": "Solar Filter",
          "description": "Enhance your viewing experience with EclipSmart Solar Filter for 8” telescopes. With two Velcro straps and four self-adhesive Velcro pads for added safety, you can be assured that the solar filter cannot be accidentally knocked off and will provide Solar Safe, ISO compliant viewing.",
          "picture": "SolarFilter.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 69,
            "nanos": 950000000
          },
          "categories": [
            "accessories",
            "telescopes"
          ]
        },
        {
          "id": "HQTGWGPNH4",
          "name": "The Comet Book",
          "description": "A 16th-century treatise on comets, created anonymously in Flanders (now northern France) and now held at the Universitätsbibliothek Kassel. Commonly known as The Comet Book (or Kometenbuch in German), its full title translates as “Comets and their General and Particular Meanings, According to Ptolomeé, Albumasar, Haly, Aliquind and other Astrologers”. The image is from https://publicdomainreview.org/collection/the-comet-book, made available by the Universitätsbibliothek Kassel under a CC-BY SA 4.0 license (https://creativecommons.org/licenses/by-sa/4.0/)",
          "picture": "TheCometBook.jpg",
          "priceUsd": {
            "currencyCode": "USD",
            "units": 0,
            "nanos": 990000000
          },
          "categories": [
            "books"
          ]
        }
      ]
    }
---
# Source: opentelemetry-demo/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
  name: grafana-clusterrole
rules: []
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
  labels:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.120.0"
    app.kubernetes.io/component: standalone-collector
rules:
  - apiGroups: [""]
    resources: ["nodes/stats", "nodes/proxy", "nodes/metrics", "pods", "namespaces"]
    verbs: ["get", "watch", "list"]
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]

---
# Source: opentelemetry-demo/charts/prometheus/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v3.1.0
    app.kubernetes.io/part-of: prometheus
  name: prometheus
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "discovery.k8s.io"
    resources:
      - endpointslices
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: opentelemetry-demo/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: grafana-clusterrolebinding
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
subjects:
  - kind: ServiceAccount
    name: grafana
    namespace: otel-demo
roleRef:
  kind: ClusterRole
  name: grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
  labels:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.120.0"
    app.kubernetes.io/component: standalone-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: otel-demo
---
# Source: opentelemetry-demo/charts/prometheus/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/component: server
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: v3.1.0
    app.kubernetes.io/part-of: prometheus
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: otel-demo
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
---
# Source: opentelemetry-demo/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: grafana
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-demo:prometheus-test
rules:
- apiGroups: [""]
  resources:
  - pods
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-demo:prometheus-test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-demo:prometheus-test
subjects:
- kind: ServiceAccount
  namespace: otel-demo
  name: default
---
# Source: opentelemetry-demo/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: grafana
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: grafana
subjects:
- kind: ServiceAccount
  name: grafana
  namespace: otel-demo
---
# Source: opentelemetry-demo/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-agent-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: jaeger-agent
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: service-agent
spec:
  clusterIP: None
  ports:
    - name: zk-compact-trft
      port: 5775
      protocol: UDP
      targetPort: 0
    - name: config-rest
      port: 5778
      targetPort: 0
    - name: jg-compact-trft
      port: 6831
      protocol: UDP
      targetPort: 0
    - name: jg-binary-trft
      port: 6832
      protocol: UDP
      targetPort: 0
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: all-in-one
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-collector-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: service-collector
spec:
  clusterIP: None
  ports:
    - name: http-zipkin
      port: 9411
      targetPort: 0
      appProtocol: http
    - name: c-tchan-trft
      port: 14267
      targetPort: 0
    - name: http-c-binary-trft
      port: 14268
      targetPort: 0
      appProtocol: http
    - name: otlp-grpc
      port: 4317
      targetPort: 0
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 0
      appProtocol: http
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: all-in-one
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-query-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: jaeger-query
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: service-query
spec:
  clusterIP: None
  ports:
    - name: http-query
      port: 16686
      targetPort: 16686
    - name: grpc-query
      port: 16685
      targetPort: 16685
  selector:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: all-in-one
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: ad
  labels:
    
    opentelemetry.io/name: ad
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: ad
    app.kubernetes.io/name: ad
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: ad
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: cart
  labels:
    
    opentelemetry.io/name: cart
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: cart
    app.kubernetes.io/name: cart
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: cart
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: checkout
  labels:
    
    opentelemetry.io/name: checkout
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: checkout
    app.kubernetes.io/name: checkout
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: checkout
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: currency
  labels:
    
    opentelemetry.io/name: currency
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: currency
    app.kubernetes.io/name: currency
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: currency
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: email
  labels:
    
    opentelemetry.io/name: email
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: email
    app.kubernetes.io/name: email
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: email
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: flagd
  labels:
    
    opentelemetry.io/name: flagd
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: flagd
    app.kubernetes.io/name: flagd
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8013
      name: rpc
      targetPort: 8013
    - port: 8016
      name: ofrep
      targetPort: 8016
    - port: 4000
      name: tcp-service-0
      targetPort: 4000
  selector:
    
    opentelemetry.io/name: flagd
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    
    opentelemetry.io/name: frontend
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: frontend
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: frontend
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-proxy
  labels:
    
    opentelemetry.io/name: frontend-proxy
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontend-proxy
    app.kubernetes.io/name: frontend-proxy
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: frontend-proxy
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: image-provider
  labels:
    
    opentelemetry.io/name: image-provider
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: image-provider
    app.kubernetes.io/name: image-provider
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8081
      name: tcp-service
      targetPort: 8081
  selector:
    
    opentelemetry.io/name: image-provider
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: kafka
  labels:
    
    opentelemetry.io/name: kafka
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: kafka
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 9092
      name: plaintext
      targetPort: 9092
    - port: 9093
      name: controller
      targetPort: 9093
  selector:
    
    opentelemetry.io/name: kafka
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: load-generator
  labels:
    
    opentelemetry.io/name: load-generator
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: load-generator
    app.kubernetes.io/name: load-generator
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8089
      name: tcp-service
      targetPort: 8089
  selector:
    
    opentelemetry.io/name: load-generator
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: payment
  labels:
    
    opentelemetry.io/name: payment
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: payment
    app.kubernetes.io/name: payment
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: payment
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: product-catalog
  labels:
    
    opentelemetry.io/name: product-catalog
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: product-catalog
    app.kubernetes.io/name: product-catalog
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: product-catalog
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: quote
  labels:
    
    opentelemetry.io/name: quote
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: quote
    app.kubernetes.io/name: quote
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: quote
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: recommendation
  labels:
    
    opentelemetry.io/name: recommendation
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: recommendation
    app.kubernetes.io/name: recommendation
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: recommendation
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: shipping
  labels:
    
    opentelemetry.io/name: shipping
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: shipping
    app.kubernetes.io/name: shipping
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    
    opentelemetry.io/name: shipping
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: valkey-cart
  labels:
    
    opentelemetry.io/name: valkey-cart
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: valkey-cart
    app.kubernetes.io/name: valkey-cart
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 6379
      name: valkey-cart
      targetPort: 6379
  selector:
    
    opentelemetry.io/name: valkey-cart
---
# Source: opentelemetry-demo/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "11.5.2"
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: opentelemetry-demo
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/version: "11.5.2"
      annotations:
        checksum/config: 99cca986c6d5f6511900d815ee5a70d0c284aeb70af56fb96108c7bf456eff87
        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
        checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
        kubectl.kubernetes.io/default-container: grafana
    spec:
      
      serviceAccountName: grafana
      automountServiceAccountToken: true
      shareProcessNamespace: false
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana
          image: "docker.io/grafana/grafana:11.5.2"
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: dashboards-default
              mountPath: "/var/lib/grafana/dashboards/default"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
            - name: config
              mountPath: "/etc/grafana/provisioning/dashboards/dashboardproviders.yaml"
              subPath: "dashboardproviders.yaml"
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
            - name: gossip-tcp
              containerPort: 9094
              protocol: TCP
            - name: gossip-udp
              containerPort: 9094
              protocol: UDP
            - name: profiling
              containerPort: 6060
              protocol: TCP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: grafana
                  key: admin-password
            - name: GF_INSTALL_PLUGINS
              valueFrom:
                configMapKeyRef:
                  name: grafana
                  key: plugins
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            limits:
              memory: 150Mi
      volumes:
        - name: config
          configMap:
            name: grafana
        - name: dashboards-default
          configMap:
            name: grafana-dashboards
        - name: storage
          emptyDir: {}
---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  labels:
    app.kubernetes.io/name: jaeger
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "1.53.0"
    app.kubernetes.io/component: all-in-one
    prometheus.io/port: "14269"
    prometheus.io/scrape: "true"
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: jaeger
      app.kubernetes.io/instance: opentelemetry-demo
      app.kubernetes.io/component: all-in-one
  template:
    metadata:
      labels:
        app.kubernetes.io/name: jaeger
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: all-in-one
      annotations:
        prometheus.io/port: "14269"
        prometheus.io/scrape: "true"
    spec:
      
      containers:
        - env:
            - name: METRICS_STORAGE_TYPE
              value: prometheus
            - name: COLLECTOR_OTLP_GRPC_HOST_PORT
              value: 0.0.0.0:4317
            - name: COLLECTOR_OTLP_HTTP_HOST_PORT
              value: 0.0.0.0:4318
            - name: SPAN_STORAGE_TYPE
              value: memory
            
            - name: COLLECTOR_ZIPKIN_HOST_PORT
              value: :9411
            - name: JAEGER_DISABLED
              value: "false"
            - name: COLLECTOR_OTLP_ENABLED
              value: "true"
          securityContext:
            {}
          image: jaegertracing/all-in-one:1.53.0
          imagePullPolicy: Always
          name: jaeger
          args:
            - "--memory.max-traces=5000"
            - "--query.base-path=/jaeger/ui"
            - "--prometheus.server-url=http://prometheus:9090"
            - "--prometheus.query.normalize-calls=true"
            - "--prometheus.query.normalize-duration=true"
          ports:
            - containerPort: 5775
              protocol: UDP
            - containerPort: 6831
              protocol: UDP
            - containerPort: 6832
              protocol: UDP
            - containerPort: 5778
              protocol: TCP
            - containerPort: 16686
              protocol: TCP
            - containerPort: 16685
              protocol: TCP
            - containerPort: 9411
              protocol: TCP
            - containerPort: 4317
              protocol: TCP
            - containerPort: 4318
              protocol: TCP
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /
              port: 14269
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 14269
              scheme: HTTP
            initialDelaySeconds: 1
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 400Mi
          volumeMounts:
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsUser: 10001
      serviceAccountName: jaeger
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: accounting
  labels:
    
    opentelemetry.io/name: accounting
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: accounting
    app.kubernetes.io/name: accounting
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: accounting
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: accounting
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: accounting
        app.kubernetes.io/name: accounting
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: accounting
          image: 'clickhouse/ch-otel-demo:latest-accounting'
          imagePullPolicy: Always
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: KAFKA_ADDR
              value: kafka:9092
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 120Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ad
  labels:
    
    opentelemetry.io/name: ad
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: ad
    app.kubernetes.io/name: ad
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: ad
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: ad
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: ad
        app.kubernetes.io/name: ad
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: ad
          image: 'clickhouse/ch-otel-demo:latest-ad'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: AD_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_LOGS_EXPORTER
              value: otlp
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cart
  labels:
    
    opentelemetry.io/name: cart
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: cart
    app.kubernetes.io/name: cart
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: cart
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: cart
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: cart
        app.kubernetes.io/name: cart
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: cart
          image: 'clickhouse/ch-otel-demo:latest-cart'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: CART_PORT
              value: "8080"
            - name: ASPNETCORE_URLS
              value: http://*:$(CART_PORT)
            - name: VALKEY_ADDR
              value: valkey-cart:6379
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 160Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 valkey-cart 6379; do echo waiting for valkey-cart; sleep 2;
            done;
          image: busybox:latest
          name: wait-for-valkey-cart
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: checkout
  labels:
    
    opentelemetry.io/name: checkout
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: checkout
    app.kubernetes.io/name: checkout
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: checkout
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: checkout
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: checkout
        app.kubernetes.io/name: checkout
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: checkout
          image: 'clickhouse/ch-otel-demo:latest-checkout'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: CHECKOUT_PORT
              value: "8080"
            - name: CART_ADDR
              value: cart:8080
            - name: CURRENCY_ADDR
              value: currency:8080
            - name: EMAIL_ADDR
              value: http://email:8080
            - name: PAYMENT_ADDR
              value: payment:8080
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: SHIPPING_ADDR
              value: shipping:8080
            - name: KAFKA_ADDR
              value: kafka:9092
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: currency
  labels:
    
    opentelemetry.io/name: currency
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: currency
    app.kubernetes.io/name: currency
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: currency
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: currency
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: currency
        app.kubernetes.io/name: currency
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: currency
          image: 'clickhouse/ch-otel-demo:latest-currency'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: CURRENCY_PORT
              value: "8080"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: VERSION
              value: '2.0.2'
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: email
  labels:
    
    opentelemetry.io/name: email
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: email
    app.kubernetes.io/name: email
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: email
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: email
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: email
        app.kubernetes.io/name: email
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: email
          image: 'clickhouse/ch-otel-demo:latest-email'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: EMAIL_PORT
              value: "8080"
            - name: APP_ENV
              value: production
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318/v1/traces
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 100Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flagd
  labels:
    
    opentelemetry.io/name: flagd
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: flagd
    app.kubernetes.io/name: flagd
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: flagd
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: flagd
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: flagd
        app.kubernetes.io/name: flagd
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: flagd
          image: 'ghcr.io/open-feature/flagd:v0.11.1'
          imagePullPolicy: Always
          command:
            - /flagd-build
            - start
            - --port
            - "8013"
            - --ofrep-port
            - "8016"
            - --uri
            - file:./etc/flagd/demo.flagd.json
          ports:
            
            - containerPort: 8013
              name: rpc
            - containerPort: 8016
              name: ofrep
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: FLAGD_METRICS_EXPORTER
              value: otel
            - name: FLAGD_OTEL_COLLECTOR_URI
              value: $(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 75Mi
          volumeMounts:
            - name: config-rw
              mountPath: /etc/flagd
        - name: flagd-ui
          image: 'clickhouse/ch-otel-demo:latest-flagd-ui'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 4000
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: FLAGD_METRICS_EXPORTER
              value: otel
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 100Mi
          volumeMounts:
            - mountPath: /app/data
              name: config-rw
      initContainers:
        - command:
          - sh
          - -c
          - cp /config-ro/demo.flagd.json /config-rw/demo.flagd.json && cat /config-rw/demo.flagd.json
          image: busybox
          name: init-config
          volumeMounts:
          - mountPath: /config-ro
            name: config-ro
          - mountPath: /config-rw
            name: config-rw
      volumes:
        - name: config-rw
          emptyDir: {}
        - configMap:
            name: flagd-config
          name: config-ro
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detection
  labels:
    
    opentelemetry.io/name: fraud-detection
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: fraud-detection
    app.kubernetes.io/name: fraud-detection
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: fraud-detection
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: fraud-detection
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: fraud-detection
        app.kubernetes.io/name: fraud-detection
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: fraud-detection
          image: 'clickhouse/ch-otel-demo:latest-fraud-detection'
          imagePullPolicy: Always
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: KAFKA_ADDR
              value: kafka:9092
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
      initContainers:
        - command:
          - sh
          - -c
          - until nc -z -v -w30 kafka 9092; do echo waiting for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    
    opentelemetry.io/name: frontend
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: frontend
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: frontend
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: frontend
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: frontend
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: frontend
          image: 'clickhouse/ch-otel-demo:latest-frontend'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: FRONTEND_PORT
              value: "8080"
            - name: FRONTEND_ADDR
              value: :8080
            - name: AD_ADDR
              value: ad:8080
            - name: CART_ADDR
              value: cart:8080
            - name: CHECKOUT_ADDR
              value: checkout:8080
            - name: CURRENCY_ADDR
              value: currency:8080
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: RECOMMENDATION_ADDR
              value: recommendation:8080
            - name: SHIPPING_ADDR
              value: shipping:8080
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: WEB_OTEL_SERVICE_NAME
              value: frontend-web
            - name: PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: http://localhost:8080/otlp-http/v1/traces
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 250Mi
          securityContext:
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-proxy
  labels:
    
    opentelemetry.io/name: frontend-proxy
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontend-proxy
    app.kubernetes.io/name: frontend-proxy
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: frontend-proxy
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: frontend-proxy
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: frontend-proxy
        app.kubernetes.io/name: frontend-proxy
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: frontend-proxy
          image: 'clickhouse/ch-otel-demo:latest-frontend-proxy'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: ENVOY_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: FLAGD_UI_HOST
              value: flagd
            - name: FLAGD_UI_PORT
              value: "4000"
            - name: FRONTEND_HOST
              value: frontend
            - name: FRONTEND_PORT
              value: "8080"
            - name: GRAFANA_HOST
              value: grafana
            - name: GRAFANA_PORT
              value: "80"
            - name: IMAGE_PROVIDER_HOST
              value: image-provider
            - name: IMAGE_PROVIDER_PORT
              value: "8081"
            - name: JAEGER_HOST
              value: jaeger-query
            - name: JAEGER_PORT
              value: "16686"
            - name: LOCUST_WEB_HOST
              value: load-generator
            - name: LOCUST_WEB_PORT
              value: "8089"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_COLLECTOR_PORT_GRPC
              value: "4317"
            - name: OTEL_COLLECTOR_PORT_HTTP
              value: "4318"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 65Mi
          securityContext:
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 101
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: image-provider
  labels:
    
    opentelemetry.io/name: image-provider
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: image-provider
    app.kubernetes.io/name: image-provider
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: image-provider
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: image-provider
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: image-provider
        app.kubernetes.io/name: image-provider
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: image-provider
          image: 'clickhouse/ch-otel-demo:latest-image-provider'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8081
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: IMAGE_PROVIDER_PORT
              value: "8081"
            - name: OTEL_COLLECTOR_PORT_GRPC
              value: "4317"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 50Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  labels:
    
    opentelemetry.io/name: kafka
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: kafka
    app.kubernetes.io/name: kafka
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: kafka
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: kafka
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: kafka
        app.kubernetes.io/name: kafka
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: kafka
          image: 'clickhouse/ch-otel-demo:latest-kafka'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 9092
              name: plaintext
            - containerPort: 9093
              name: controller
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: KAFKA_ADVERTISED_LISTENERS
              value: PLAINTEXT://kafka:9092
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: KAFKA_HEAP_OPTS
              value: -Xmx400M -Xms400M
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 600Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  labels:
    
    opentelemetry.io/name: load-generator
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: load-generator
    app.kubernetes.io/name: load-generator
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: load-generator
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: load-generator
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: load-generator
        app.kubernetes.io/name: load-generator
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: load-generator
          image: 'clickhouse/ch-otel-demo:latest-load-generator'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8089
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: LOCUST_WEB_HOST
              value: 0.0.0.0
            - name: LOCUST_WEB_PORT
              value: "8089"
            - name: LOCUST_USERS
              value: "10"
            - name: LOCUST_SPAWN_RATE
              value: "1"
            - name: LOCUST_HOST
              value: http://frontend-proxy:8080
            - name: LOCUST_HEADLESS
              value: "false"
            - name: LOCUST_AUTOSTART
              value: "true"
            - name: LOCUST_BROWSER_TRAFFIC_ENABLED
              value: "true"
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_OFREP_PORT
              value: "8016"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 1500Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: payment
  labels:
    
    opentelemetry.io/name: payment
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: payment
    app.kubernetes.io/name: payment
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: payment
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: payment
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: payment
        app.kubernetes.io/name: payment
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: payment
          image: 'clickhouse/ch-otel-demo:latest-payment'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: PAYMENT_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 120Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-catalog
  labels:
    
    opentelemetry.io/name: product-catalog
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: product-catalog
    app.kubernetes.io/name: product-catalog
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: product-catalog
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: product-catalog
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: product-catalog
        app.kubernetes.io/name: product-catalog
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: product-catalog
          image: 'clickhouse/ch-otel-demo:latest-product-catalog'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: PRODUCT_CATALOG_PORT
              value: "8080"
            - name: PRODUCT_CATALOG_RELOAD_INTERVAL
              value: "10"
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
            - name: product-catalog-products
              mountPath: /usr/src/app/products
      volumes:
        - name: product-catalog-products
          configMap:
            name: product-catalog-products
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: quote
  labels:
    
    opentelemetry.io/name: quote
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: quote
    app.kubernetes.io/name: quote
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: quote
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: quote
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: quote
        app.kubernetes.io/name: quote
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: quote
          image: 'clickhouse/ch-otel-demo:latest-quote'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: QUOTE_PORT
              value: "8080"
            - name: OTEL_PHP_AUTOLOAD_ENABLED
              value: "true"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 40Mi
          securityContext:
            runAsGroup: 33
            runAsNonRoot: true
            runAsUser: 33
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation
  labels:
    
    opentelemetry.io/name: recommendation
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: recommendation
    app.kubernetes.io/name: recommendation
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: recommendation
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: recommendation
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: recommendation
        app.kubernetes.io/name: recommendation
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: recommendation
          image: 'clickhouse/ch-otel-demo:latest-recommendation'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: RECOMMENDATION_PORT
              value: "8080"
            - name: PRODUCT_CATALOG_ADDR
              value: product-catalog:8080
            - name: OTEL_PYTHON_LOG_CORRELATION
              value: "true"
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: FLAGD_HOST
              value: flagd
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 500Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shipping
  labels:
    
    opentelemetry.io/name: shipping
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: shipping
    app.kubernetes.io/name: shipping
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: shipping
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: shipping
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: shipping
        app.kubernetes.io/name: shipping
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: shipping
          image: 'clickhouse/ch-otel-demo:latest-shipping'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: SHIPPING_PORT
              value: "8080"
            - name: QUOTE_ADDR
              value: http://quote:8080
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: valkey-cart
  labels:
    
    opentelemetry.io/name: valkey-cart
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: valkey-cart
    app.kubernetes.io/name: valkey-cart
    app.kubernetes.io/version: "2.0.2"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      
      opentelemetry.io/name: valkey-cart
  template:
    metadata:
      labels:
        
        opentelemetry.io/name: valkey-cart
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: valkey-cart
        app.kubernetes.io/name: valkey-cart
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: valkey-cart
          image: 'valkey/valkey:7.2-alpine'
          imagePullPolicy: Always
          ports:
            
            - containerPort: 6379
              name: valkey-cart
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: otel-collector
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2
          resources:
            limits:
              memory: 20Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 999
          volumeMounts:
      volumes:
---
# ConfigMap for ClickHouse configuration files
apiVersion: v1
kind: ConfigMap
metadata:
  name: clickhouse-config
data:
  config.xml: |
    <?xml version="1.0"?>
    <clickhouse>
        <logger>
            <level>debug</level>
            <console>true</console>
            <log remove="remove"/>
            <errorlog remove="remove"/>
        </logger>

        <listen_host>0.0.0.0</listen_host>
        <http_port>8123</http_port>
        <tcp_port>9000</tcp_port>
        <interserver_http_host>ch-server</interserver_http_host>
        <interserver_http_port>9009</interserver_http_port>

        <max_connections>4096</max_connections>
        <keep_alive_timeout>64</keep_alive_timeout>
        <max_concurrent_queries>100</max_concurrent_queries>
        <uncompressed_cache_size>8589934592</uncompressed_cache_size>
        <mark_cache_size>5368709120</mark_cache_size>

        <path>/var/lib/clickhouse/</path>
        <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
        <user_files_path>/var/lib/clickhouse/user_files/</user_files_path>

        <users_config>users.xml</users_config>
        <default_profile>default</default_profile>
        <default_database>default</default_database>
        <timezone>UTC</timezone>
        <mlock_executable>false</mlock_executable>

        <!-- Prometheus exporter -->
        <prometheus>
            <endpoint>/metrics</endpoint>
            <port>9363</port>
            <metrics>true</metrics>
            <events>true</events>
            <asynchronous_metrics>true</asynchronous_metrics>
            <errors>true</errors>
        </prometheus>

        <!-- Query log. Used only for queries with setting log_queries = 1. -->
        <query_log>
            <database>system</database>
            <table>query_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>

        <!-- Metric log contains rows with current values of ProfileEvents, CurrentMetrics collected with "collect_interval_milliseconds" interval. -->
        <metric_log>
            <database>system</database>
            <table>metric_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
            <collect_interval_milliseconds>1000</collect_interval_milliseconds>
        </metric_log>

        <!--
            Asynchronous metric log contains values of metrics from
            system.asynchronous_metrics.
        -->
        <asynchronous_metric_log>
            <database>system</database>
            <table>asynchronous_metric_log</table>
            <!--
                Asynchronous metrics are updated once a minute, so there is
                no need to flush more often.
            -->
            <flush_interval_milliseconds>7000</flush_interval_milliseconds>
        </asynchronous_metric_log>

        <!--
            OpenTelemetry log contains OpenTelemetry trace spans.
        -->
        <opentelemetry_span_log>
            <!--
                The default table creation code is insufficient, this <engine> spec
                is a workaround. There is no 'event_time' for this log, but two times,
                start and finish. It is sorted by finish time, to avoid inserting
                data too far away in the past (probably we can sometimes insert a span
                that is seconds earlier than the last span in the table, due to a race
                between several spans inserted in parallel). This gives the spans a
                global order that we can use to e.g. retry insertion into some external
                system.
            -->
            <engine>
                engine MergeTree
                partition by toYYYYMM(finish_date)
                order by (finish_date, finish_time_us, trace_id)
            </engine>
            <database>system</database>
            <table>opentelemetry_span_log</table>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </opentelemetry_span_log>


        <!-- Crash log. Stores stack traces for fatal errors.
            This table is normally empty. -->
        <crash_log>
            <database>system</database>
            <table>crash_log</table>

            <partition_by />
            <flush_interval_milliseconds>1000</flush_interval_milliseconds>
        </crash_log>

        <!-- Profiling on Processors level. -->
        <processors_profile_log>
            <database>system</database>
            <table>processors_profile_log</table>

            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </processors_profile_log>

        <!-- Uncomment if use part log.
            Part log contains information about all actions with parts in MergeTree tables (creation, deletion, merges, downloads).-->
        <part_log>
            <database>system</database>
            <table>part_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </part_log>

        <!-- Trace log. Stores stack traces collected by query profilers.
            See query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings. -->
        <trace_log>
            <database>system</database>
            <table>trace_log</table>

            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </trace_log>

        <!-- Query thread log. Has information about all threads participated in query execution.
            Used only for queries with setting log_query_threads = 1. -->
        <query_thread_log>
            <database>system</database>
            <table>query_thread_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_thread_log>

        <!-- Query views log. Has information about all dependent views associated with a query.
            Used only for queries with setting log_query_views = 1. -->
        <query_views_log>
            <database>system</database>
            <table>query_views_log</table>
            <partition_by>toYYYYMM(event_date)</partition_by>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_views_log>

        <remote_servers>
            <hdx_cluster>
                <shard>
                    <replica>
                        <host>ch-server</host>
                        <port>9000</port>
                    </replica>
                </shard>
            </hdx_cluster>
        </remote_servers>

        <distributed_ddl>
            <path>/clickhouse/task_queue/ddl</path>
        </distributed_ddl>

        <format_schema_path>/var/lib/clickhouse/format_schemas/</format_schema_path>
    </clickhouse>
  users.xml: |
    <?xml version="1.0"?>
    <clickhouse>
        <profiles>
            <default>
                <max_memory_usage>10000000000</max_memory_usage>
                <use_uncompressed_cache>0</use_uncompressed_cache>
                <load_balancing>in_order</load_balancing>
                <log_queries>1</log_queries>
            </default>
        </profiles>
        <users>
            <default>
                <password></password>
                <profile>default</profile>
                <networks>
                    <ip>::/0</ip>
                </networks>
                <quota>default</quota>
            </default>
            <api>
                <password>api</password>
                <profile>default</profile>
                <networks>
                    <ip>::/0</ip>
                </networks>
                <quota>default</quota>
            </api>
            <worker>
                <password>worker</password>
                <profile>default</profile>
                <networks>
                    <ip>::/0</ip>
                </networks>
                <quota>default</quota>
            </worker>
        </users>
        <quotas>
            <default>
                <interval>
                    <duration>3600</duration>
                    <queries>0</queries>
                    <errors>0</errors>
                    <result_rows>0</result_rows>
                    <read_rows>0</read_rows>
                    <execution_time>0</execution_time>
                </interval>
            </default>
        </quotas>
    </clickhouse>
---
# PersistentVolumeClaim for ClickHouse data
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: clickhouse-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # Adjust size as needed
---
# PersistentVolumeClaim for ClickHouse logs
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: clickhouse-logs-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi  # Adjust size as needed
---
# Deployment for ClickHouse
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clickhouse
spec:
  replicas: 1
  selector:
    matchLabels:
      app: clickhouse
  template:
    metadata:
      labels:
        app: clickhouse
    spec:
      containers:
      - name: clickhouse
        image: clickhouse/clickhouse-server:24-alpine
        ports:
        - containerPort: 8123
          name: http
        - containerPort: 9000
          name: native
        env:
        - name: CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT
          value: "1"
        volumeMounts:
        - name: config-volume
          mountPath: /etc/clickhouse-server/config.xml
          subPath: config.xml
        - name: config-volume
          mountPath: /etc/clickhouse-server/users.xml
          subPath: users.xml
        - name: data-volume
          mountPath: /var/lib/clickhouse
        - name: logs-volume
          mountPath: /var/log/clickhouse-server
        livenessProbe:
          httpGet:
            path: /ping
            port: 8123
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ping
            port: 8123
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
      volumes:
      - name: config-volume
        configMap:
          name: clickhouse-config
      - name: data-volume
        persistentVolumeClaim:
          claimName: clickhouse-data-pvc
      - name: logs-volume
        persistentVolumeClaim:
          claimName: clickhouse-logs-pvc
---
# Service for ClickHouse
apiVersion: v1
kind: Service
metadata:
  name: clickhouse
spec:
  selector:
    app: clickhouse
  ports:
  - name: http
    port: 8123
    targetPort: 8123
  - name: native
    port: 9000
    targetPort: 9000
  type: ClusterIP
---
# MongoDB Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperdx-db
  labels:
    app: hyperdx-db
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hyperdx-db
  template:
    metadata:
      labels:
        app: hyperdx-db
    spec:
      containers:
      - name: mongodb
        image: mongo:5.0.14-focal
        ports:
        - containerPort: 27017
          name: mongodb
        volumeMounts:
        - name: mongodb-data
          mountPath: /data/db
      volumes:
      - name: mongodb-data
        persistentVolumeClaim:
          claimName: mongodb-data-pvc
---
# MongoDB Service
apiVersion: v1
kind: Service
metadata:
  name: hyperdx-db
spec:
  selector:
    app: hyperdx-db
  ports:
  - name: mongodb
    port: 27017
    targetPort: 27017
  type: ClusterIP
---
# MongoDB PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi  # Adjust size as needed


---
# HyperDX ConfigMap for environment variables
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyperdx-config
data:
  HYPERDX_LOG_LEVEL: "info"  # Default value, adjust as needed
  ENVOY_PORT: "8080"  # Default value, adjust as needed
  HOST_FILESYSTEM: "/var/log"  # Default value, adjust as needed
  OTEL_COLLECTOR_PORT_GRPC: "4317"
  OTEL_COLLECTOR_PORT_HTTP: "4318"
  HYPERDX_API_PORT: "8000"  # Default value, adjust as needed
  HYPERDX_APP_PORT: "9080"  # Default value, adjust as needed
  HYPERDX_APP_URL: "http://localhost"  # Default value, adjust as needed
  FRONTEND_URL: "http://localhost:9080"  # Default value, adjust as needed
  USAGE_STATS_ENABLED: "true"  # Default value, adjust as needed
---
# HyperDX Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hyperdx
  labels:
    app: hyperdx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hyperdx

  template:
    metadata:
      labels:
        app: hyperdx
    spec:
      containers:
      - name: hyperdx
        image: docker.hyperdx.io/hyperdx/hyperdx:2-beta.16
        resources:
          requests:
            cpu: "500m"     
            memory: "512Mi" 
          limits:
            cpu: "1000m"    
            memory: "1Gi"
        ports:
        - containerPort: 4318
          name: otlp
        - containerPort: 8000
          name: api
        - containerPort: 9080
          name: app
        env:
        - name: FRONTEND_URL
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: FRONTEND_URL
        - name: HYPERDX_API_KEY
          value: ""
        - name: HYPERDX_API_PORT
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: HYPERDX_API_PORT
        - name: HYPERDX_APP_PORT
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: HYPERDX_APP_PORT
        - name: HYPERDX_APP_URL
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: HYPERDX_APP_URL
        - name: HYPERDX_LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: HYPERDX_LOG_LEVEL
        - name: MINER_API_URL
          value: "http://miner:5123"
        - name: MONGO_URI
          value: "mongodb://hyperdx-db:27017/hyperdx"
        - name: NEXT_PUBLIC_SERVER_URL
          value: "http://127.0.0.1:8000"
        - name: OTEL_SERVICE_NAME
          value: "hdx-oss-api"
        - name: USAGE_STATS_ENABLED
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: USAGE_STATS_ENABLED
---
# HyperDX Service
apiVersion: v1
kind: Service
metadata:
  name: hyperdx
spec:
  selector:
    app: hyperdx
  ports:
  - name: otlp
    port: 4318
    targetPort: 4318
  - name: api
    port: 8000
    targetPort: 8000
  - name: app
    port: 9080
    targetPort: 9080
  type: ClusterIP
---
# OpenTelemetry Collector ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config-extras
data:
  config-extras.yaml: |
    # Copyright The OpenTelemetry Authors
    # SPDX-License-Identifier: Apache-2.0

    # extra settings to be merged into OpenTelemetry Collector configuration
    # do not delete this file

    receivers:
      httpcheck/frontend-proxy:
        targets:
          - endpoint: http://frontend-proxy:${env:ENVOY_PORT}
      redis:
        endpoint: "valkey-cart:6379"
        username: "valkey"
        collection_interval: 10s
    
    processors:
      transform:
        error_mode: ignore
        trace_statements:
          - context: span
            statements:
              # could be removed when https://github.com/vercel/next.js/pull/64852 is fixed upstream
              - replace_pattern(name, "\\?.*", "")
              - replace_match(name, "GET /api/products/*", "GET /api/products/{productId}")
    service:
      pipelines:
          metrics:
            receivers: [otlp]
            processors: [memory_limiter, batch]
            exporters: [clickhouse]
---
# OpenTelemetry Collector Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.120.0"
    app.kubernetes.io/component: standalone-collector
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: opentelemetry-demo
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/instance: opentelemetry-demo
        component: standalone-collector
    spec:
      serviceAccountName: otel-collector
      securityContext:
        {}
      containers:
      - name: opentelemetry-collector
        image: ghcr.io/hyperdxio/hyperdx-otel-collector:2-beta.16
        args:
        - "--config=/etc/otelcol-contrib/config.yaml"
        - "--config=/etc/otelcol-contrib/config-extras.yaml"
        securityContext:
          {}
        ports:
        - containerPort: 13133
          name: health-check
        - containerPort: 24225
          name: fluentd
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 8888
          name: metrics
        env:
        - name: CLICKHOUSE_SERVER_ENDPOINT
          value: "clickhouse:9000"
        - name: HYPERDX_LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: HYPERDX_LOG_LEVEL
        - name: ENVOY_PORT
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: ENVOY_PORT
        - name: HOST_FILESYSTEM
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: HOST_FILESYSTEM
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: OTEL_COLLECTOR_PORT_GRPC
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: OTEL_COLLECTOR_PORT_GRPC
        - name: OTEL_COLLECTOR_PORT_HTTP
          valueFrom:
            configMapKeyRef:
              name: hyperdx-config
              key: OTEL_COLLECTOR_PORT_HTTP
        - name: GOMEMLIMIT
          value: "160MiB"
        - name: CLICKHOUSE_PASSWORD
          value: ""
        - name: CLICKHOUSE_USER
          value: "default"
        livenessProbe:
          httpGet:
            path: /
            port: 13133
        readinessProbe:
          httpGet:
            path: /
            port: 13133
        resources:
          limits:
            memory: "200Mi"
        volumeMounts:
        - name: config-extras
          mountPath: /etc/otelcol-contrib/config-extras.yaml
          subPath: config-extras.yaml
        securityContext:
          runAsUser: 0
          runAsGroup: 0
      volumes:
      - name: config-extras
        configMap:
          name: otel-collector-config-extras
---
# OpenTelemetry Collector Service
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: otel-demo
  labels:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/version: "0.120.0"
    app.kubernetes.io/component: standalone-collector
    component: standalone-collector
spec:
  type: ClusterIP
  ports:
  - name: health-check
    port: 13133
    targetPort: 13133
  - name: fluentd
    port: 24225
    targetPort: 24225
  - name: otlp
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
  - name: metrics
    port: 8888
    targetPort: 8888
    protocol: TCP
  selector:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: opentelemetry-demo
    component: standalone-collector
  internalTrafficPolicy: Cluster

